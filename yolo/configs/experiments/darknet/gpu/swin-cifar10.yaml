runtime:
  distribution_strategy: 'mirrored'
  mixed_precision_dtype: 'float16'
  num_gpus: 1
task:
  model:
    num_classes: 10
    input_size: [32, 32, 3]
    backbone:
      type: 'swin'
      swin:
        min_level: null
        max_level: null
        patch_size: 2
        embed_dims: 64 
        window_size: [8, 8] 
        depths: [2, 2]
        num_heads: [4, 4]
        drop_path: 0.0
        absolute_positional_embed: False
        normalize_endpoints: False
        mlp_ratio: 2
    norm_activation:
      activation: 'gelu'
  losses:
    l2_weight_decay: 0.00
    one_hot: true
    label_smoothing: 0.0
  train_data:
    tfds_name: cifar10
    tfds_data_dir: '/media/vbanna/DATA_SHARE/CV/datasets/tensorflow/'
    tfds_split: train
    is_training: true
    global_batch_size: 64
    dtype: 'float16'
    aug_type: null
    aug_policy: null
  validation_data:
    tfds_name: cifar10
    tfds_data_dir: '/media/vbanna/DATA_SHARE/CV/datasets/tensorflow/'
    tfds_split: test
    is_training: false
    global_batch_size: 32
    dtype: 'float16'
    drop_remainder: true
trainer:
  train_steps: 10000  # epochs: 120
  validation_steps: 312  # size of validation data
  validation_interval: 1000
  steps_per_loop: 1000
  summary_interval: 1000
  checkpoint_interval: 1000
  optimizer_config:
    # ema: 
    #   average_decay: 0.9999
    #   trainable_weights_only: False
    #   dynamic_decay: True
    optimizer:
      type: 'adam'
      adam:
        name: adam
    learning_rate:
      type: cosine
      cosine:
        initial_learning_rate: 0.001
        decay_steps: 100000000000
    warmup:
      type: 'linear'
      linear:
        warmup_steps: 1000  # learning rate rises from 0 to 0.1 over 1000 steps