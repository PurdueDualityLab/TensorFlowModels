runtime:
  distribution_strategy: 'tpu'
  mixed_precision_dtype: 'bfloat16'
task:
  model:
    num_classes: 1001
    input_size: [224, 224, 3]
    backbone:
      type: 'swin'
      swin:
        min_level: 3
        max_level: 5
        patch_size: 4
        embed_dims: 96 
        window_size: [7, 7, 7, 7] 
        depths: [2, 2, 6, 2]
        num_heads: [3, 6, 12, 24]
        drop_path: 0.2
        absolute_positional_embed: False
    norm_activation:
      activation: 'gelu'
  losses:
    l2_weight_decay: 0.00
    one_hot: true
    label_smoothing: 0.1
  train_data:
    tfds_name: imagenet2012
    tfds_data_dir: 'gs://tensorflow2/tensorflow_datasets/'
    tfds_split: train
    is_training: true
    global_batch_size: 1024
    dtype: 'bfloat16'
  validation_data:
    tfds_name: imagenet2012
    tfds_data_dir: 'gs://tensorflow2/tensorflow_datasets/'
    tfds_split: validation
    is_training: false
    global_batch_size: 1024
    dtype: 'bfloat16'
    drop_remainder: true
trainer:
  train_steps: 351563  # epochs: 120
  validation_steps: 48  # size of validation data
  validation_interval: 5000
  steps_per_loop: 1000
  summary_interval: 1000
  checkpoint_interval: 1000
  optimizer_config:
    optimizer:
      type: 'adamw'
      adamw:
        name: AdamWeightDecay
        beta_1: 0.9
        beta_2: 0.999
        weight_decay_rate: 0.05
        include_in_weight_decay: ['kernel']
        exclude_from_weight_decay: ['(.)*']
        gradient_clip_norm: 1.0
    learning_rate:
      type: cosine
      cosine:
        initial_learning_rate: 0.001
        decay_steps: 351563
    warmup:
      type: 'linear'
      linear:
        warmup_steps: 23348  # learning rate rises from 0 to 0.1 over 1000 steps